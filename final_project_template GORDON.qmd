---
title: "Final Project Template"
author: Dr. Petersen
output: pdf_document
---

<style type="text/css">
h1.title {
font-size: 40px;
text-align: center;
}
h4.author {
font-size: 40px;
text-align: center;
}
</style>

\newpage

```{r setup, include=FALSE}
library(tidyverse)
library(vroom)
library(glmnet)
library(corrplot)  # for the correlation matrix
library(bestglm)  # for variable selection
library(car)  # for the VIFs
library(pROC)  # for the ROC curve
library(ROCR)  # for the color-coded ROC curve
library(ggfortify)
library(patchwork)
```


``` {r, message = FALSE}
df <- vroom("./Cancer_Data.csv")
```

# Abstract

Here, you will briefly summarize the entire report, including the data used, questions of interest, and relevant findings.

# 1 Problem and Motivation


## 1.1 Data Description

Millions of people each year are diagnosed with cancer; most people know someone who has it. One very important part of cancer is whether or not it is dangerous. In other words, if the cancer is benign (harmless) or malignant (harmful). This study was performed by _____ and their goal was to gather data on a number of predictors that can help in predicting whether or not a cancer is benign or malignant. To start out with, we removed some variables that we did not want to test. The variables that we kept were the ones that measured the mean. We got rid of everything that measured the "worst" of a predictor and the standard error of the predictor. We did this not only to help with interpretability (it is a lot easier to understand how the mean area of a tumor is associated with the response rather than the standard error of its area), but also because we were only curious about how those certain variables were related to a tumor being benign or malignant. There also was not clear documentation on what "worst" meant in the context of this data set, so that is another reason we removed those variables. After doing some further variable selection using the LASSO Shrinkage Method, the variables that we will be using in this project are radius_mean, texture_mean, concave points_mean, smoothness_mean, and symmetry_mean. Radius_mean is the average radius taken from the center to the edges of the tumor along multiple different lines. Texture_mean is a normalized measurement of how textured a tumor is. Concave points_mean is an average measure of the number of concave points in a given portion of the tumor. Smoothness_mean measures how smooth the boundary of the tumor is. Lastly, symmetry_mean is a measure of how symmetrical a tumor is, which is done by splitting the tumor in half and seeing how closely it resembles its mirror counterpart.

## 1.2 Questions of Interest

Can we predict whether a tumor is malignant or benign based on the data provided?
Does the mean radius of a tumor have a significant relationship with its status as benign or malignant?

## 1.3 Regression Methods

To answer the first question about prediction, we will mostly be relying on logistic regression and the techniques we learned during Module 7. We will also be using some predictive analytic techniques to see how well we are able to predict the malignancy of tumors. To answer the second question, we will also  be using logistic regression, specifically the output of our Generalized Logistic Regression model. We will be looking at the p-value of radius_mean to determine the answer to this question.

# 2 Analyses, Results, and Interpretation

```{r}
df <- df %>%
  mutate(diagnosis = as.factor(diagnosis)) %>%
  select(diagnosis, radius_mean, texture_mean, perimeter_mean, area_mean, smoothness_mean, compactness_mean, concavity_mean, `concave points_mean`, symmetry_mean, fractal_dimension_mean)
```

## Exploratory Data Analysis 
```{r}
create_boxplot <- function(predictor) {
  plot <- ggplot(data = df) +
    geom_boxplot(mapping = aes(y = predictor, x = diagnosis)) +
    coord_flip()
  plot
}

plot1 <- create_boxplot(df$radius_mean)
plot2 <- create_boxplot(df$texture_mean)
plot3 <- create_boxplot(df$perimeter_mean)
plot4 <- create_boxplot(df$area_mean)
plot5 <- create_boxplot(df$smoothness_mean)
plot6 <- create_boxplot(df$compactness_mean)
plot7 <- create_boxplot(df$concavity_mean)
plot8 <- create_boxplot(df$`concave points_mean`)
plot9 <- create_boxplot(df$symmetry_mean)
plot10 <- create_boxplot(df$fractal_dimension_mean)

plot1 + plot2 + plot3 + plot4 + plot5 + plot6 + plot7 + plot8 + plot9 + plot10
```

## No multicollinearity (using VIFs)
```{r}
df.lm = glm(diagnosis ~ ., data = df, family = binomial(link = 'logit'))
corrplot(cor(select(df, -diagnosis)), type = 'upper')
vif(df.lm)
```


## LASSO Shrinkage Method

```{r}
set.seed(38)

df_x <- as.matrix(select(df, -diagnosis))
df_y <- unlist(select(df, diagnosis) %>%
                  mutate(diagnosis = ifelse(diagnosis == 'M', 1, 0)))

df_LASSO_cv <- cv.glmnet(x = df_x, 
                          y = df_y, 
                          type.measure = "auc", 
                          alpha = 1, 
                          family = 'binomial')

autoplot(df_LASSO_cv, label = FALSE) +
  theme_bw() +
  theme(aspect.ratio = 1)

coef(df_LASSO_cv, s = "lambda.1se")
```

## Fit a new model
```{r}
df_logistic <- glm(diagnosis ~ radius_mean + texture_mean + `concave points_mean`, 
                   data = df,
                   family = binomial(link = "logit"))
summary(df_logistic)
```

$\log(\frac{\pi_i}{1-\pi_i})=-21.164 + 0.656\times \text{radius mean}_i + 0.326\times \text{texture mean}_i + 101.166\times \text{texture_mean}_i$

Where $\pi_i = P(diagnosis_i = 1 | \text{radius mean}_i, \text{texture mean}_i, \text{texture_mean}_i)$ and $diagnosis_i \stackrel{iid}{\sim} Bernoulli(\pi_i)$

## No multicollinearity (using VIFs)
```{r}
# real R^2
df_lm <- glm(diagnosis ~ radius_mean + texture_mean + `concave points_mean`, 
             data = mutate(df, diagnosis = ifelse(diagnosis == 'M', 1, 0)))

vif(df_lm)
max(vif(df_lm))  # less than 10
mean(vif(df_lm))  # less than 5
```

Our Variance Inflation Factors (VIFs) have a mean less than 5 and a maximum less than 10. We may assume there is no extreme multicollinearity in our model.

## The x's vs log odds are linear (monotone in probability)

```{r}
df_numeric <- mutate(df, diagnosis = ifelse(diagnosis == 'B', 0, 1))

scatter.smooth(x = df_numeric$radius_mean, y = df_numeric$diagnosis + .0001)
scatter.smooth(x = df$texture_mean, y = df_numeric$diagnosis)
scatter.smooth(x = df$`concave points_mean`, y = df_numeric$diagnosis)
```

None of our plots show extreme non-linearity. We may assume the x's vs log odds are linear.


## The observations are independent
Each of the observations in our dataset has a unique ID number and is assumed to have been randomly collected. We will assume that our observations are independent.


## The response variable is Bernoulli
Our response variable is modeled by a Bernoulli distribution, with the only possible outcomes being Malignant (True) and Benign (False). 


## No counfounding variables
We will assume that we have accounted for all variables of significance in our model.


## Influential observations: 
```{r}
plot(df_logistic, which = 5)  # residuals vs leverage
```

None of our observations appear unduly influential based on our Cook's plot.


All of our assumptions are met. We may proceed to statistical inference.


Conduct the analysis in a logical fashion, including necessary code (and not any superfluous code) and its output.  Give simple interpretations of your findings related to the questions of interest.

# 3 Conclusions

Give additional context to you results and interpretations from Section 5 with respect to the data set and its purpose.  Describe any potential weaknesses in your analysis, and how they may affect the reliability of the results.

# 4 Contributions

Ty Hawkes took the lead in finding and cleaning our dataset. He also consistently reviewed code chunks and was dedicated to debugging any issues. Ty wrote the interpretations and generally helped the group out when we were stuck. 

Talmage Hilton wrote most of our conclusions and observed results. He was focused on the goals of the project and kept us on track. He also made the confusion matrix and ROC curve to attest to the accuracy of our model.

Chris Gordon checked the assumptions for our model and helped with debugging. He also wrote our fitted model and helped find solutions to problems at each step of the project. 

## APPENDIX

If there are any analyses, such as exploratory plots/summaries or work you did to determine appropriate transformations, or any relevant material that does not belong in one of the sections above, these can go in the Appendix.  If you don't have any material like this, you can remove this section.


